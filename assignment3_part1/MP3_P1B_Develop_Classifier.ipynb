{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Part 1: Developing Your Own Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import gc\n",
    "\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import average_precision_score\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from kaggle_submission import output_submission_csv\n",
    "from classifier import SimpleClassifier, ConvNet\n",
    "from voc_dataloader import VocDataset, VOC_CLASSES\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1B: Design your own network\n",
    "\n",
    "In this notebook, your task is to create and train your own model for multi-label classification on VOC Pascal.\n",
    "\n",
    "## What to do\n",
    "1. You will make change on network architecture in ```classifier.py```.\n",
    "2. You may also want to change other hyperparameters to assist your training to get a better performances. Hints will be given in the below instructions.\n",
    "\n",
    "## What to submit\n",
    "Check the submission template for details what to submit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(train_loader, classifier, criterion, optimizer):\n",
    "    classifier.train()\n",
    "    loss_ = 0.0\n",
    "    losses = []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = classifier(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss)\n",
    "    return torch.stack(losses).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(test_loader, classifier, criterion, print_ind_classes=True, print_total=True):\n",
    "    classifier.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        y_true = np.zeros((0,21))\n",
    "        y_score = np.zeros((0,21))\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = classifier(images)\n",
    "            y_true = np.concatenate((y_true, labels.cpu().numpy()), axis=0)\n",
    "            y_score = np.concatenate((y_score, logits.cpu().numpy()), axis=0)\n",
    "            loss = criterion(logits, labels)\n",
    "            losses.append(loss.item())\n",
    "        aps = []\n",
    "        # ignore first class which is background\n",
    "        for i in range(1, y_true.shape[1]):\n",
    "            ap = average_precision_score(y_true[:, i], y_score[:, i])\n",
    "            if print_ind_classes:\n",
    "                print('-------  Class: {:<12}     AP: {:>8.4f}  -------'.format(VOC_CLASSES[i], ap))\n",
    "            aps.append(ap)\n",
    "        \n",
    "        mAP = np.mean(aps)\n",
    "        test_loss = np.mean(losses)\n",
    "        if print_total:\n",
    "            print('mAP: {0:.4f}'.format(mAP))\n",
    "            print('Avg loss: {}'.format(test_loss))\n",
    "        \n",
    "    return mAP, test_loss, aps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train, val, test_frequency, num_epochs):\n",
    "    plt.plot(train, label=\"train\")\n",
    "    indices = [i for i in range(num_epochs) if ((i+1)%test_frequency == 0 or i ==0)]\n",
    "    plt.plot(indices, val, label=\"val\")\n",
    "    plt.title(\"Loss Plot\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_mAP(train, val, test_frequency, num_epochs):\n",
    "    indices = [i for i in range(num_epochs) if ((i+1)%test_frequency == 0 or i ==0)]\n",
    "    plt.plot(indices, train, label=\"train\")\n",
    "    plt.plot(indices, val, label=\"val\")\n",
    "    plt.title(\"mAP Plot\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_checkpoint(path):\n",
    "  torch.save({\n",
    "    'epoch': 0,\n",
    "    'model_state_dict': None,\n",
    "    'train_losses': [],\n",
    "    'train_mAPs': [],\n",
    "    'val_losses': [],\n",
    "    'val_mAPs': [],\n",
    "  }, path)\n",
    "\n",
    "def save_checkpoint(path, epoch, state_dict, train_losses, train_mAPs, val_losses, val_mAPs):\n",
    "  torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': state_dict,\n",
    "    'train_losses': train_losses,\n",
    "    'train_mAPs': train_mAPs,\n",
    "    'val_losses': val_losses,\n",
    "    'val_mAPs': val_mAPs,\n",
    "  }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(path, classifier, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency=5):\n",
    "    checkpoint = torch.load(path)\n",
    "\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    train_mAPs = checkpoint['train_mAPs']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    val_mAPs = checkpoint['val_mAPs']\n",
    "    initial_epoch = checkpoint['epoch']\n",
    "\n",
    "    if checkpoint['model_state_dict'] is not None:\n",
    "        classifier.load_state_dict(checkpoint['model_state_dict'])  \n",
    "\n",
    "    for epoch in range(1, initial_epoch + 1):\n",
    "        print(\"Loss for Training on Epoch \" + str(epoch) + \" is \"+ str(train_losses[epoch-1]))\n",
    "\n",
    "    for epoch in range(initial_epoch + 1, num_epochs + 1):\n",
    "        print(\"Starting epoch number \" + str(epoch))\n",
    "        train_loss = train_classifier(train_loader, classifier, criterion, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "        print(\"Loss for Training on Epoch \" +str(epoch) + \" is \"+ str(train_loss))\n",
    "\n",
    "        if (epoch % test_frequency == 0 or epoch == 1):\n",
    "            mAP_train, _, _ = test_classifier(train_loader, classifier, criterion, False, False)\n",
    "            train_mAPs.append(mAP_train)\n",
    "            mAP_val, val_loss, _ = test_classifier(val_loader, classifier, criterion)\n",
    "            print('Evaluating classifier')\n",
    "            print(\"Mean Precision Score for Testing on Epoch \" +str(epoch) + \" is \"+ str(mAP_val))\n",
    "            val_losses.append(val_loss)\n",
    "            val_mAPs.append(mAP_val)\n",
    "\n",
    "        # Save training state\n",
    "        save_checkpoint(\n",
    "            path, \n",
    "            epoch, \n",
    "            classifier.state_dict(),\n",
    "            train_losses,\n",
    "            train_mAPs,\n",
    "            val_losses,\n",
    "            val_mAPs\n",
    "        )\n",
    "    \n",
    "    return classifier, train_losses, val_losses, train_mAPs, val_mAPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Your Own Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "To meet the benchmark for this assignment you will need to improve the network. Note you should have noticed pretrained Alenxt performs really well, but training Alexnet from scratch performs much worse. We hope you can design a better architecture over both the simple classifier and AlexNet to train from scratch.\n",
    "\n",
    "### How to start\n",
    "You may take inspiration from other published architectures and architectures discussed in lecture. However, you are NOT allowed to use predefined models (e.g. models from torchvision) or use pretrained weights. Training must be done from scratch with your own custom model.\n",
    "\n",
    "#### Some hints\n",
    "There are a variety of different approaches you should try to improve performance from the simple classifier:\n",
    "\n",
    "* Network architecture changes\n",
    "    * Number of layers: try adding layers to make your network deeper\n",
    "    * Batch normalization: adding batch norm between layers will likely give you a significant performance increase\n",
    "    * Residual connections: as you increase the depth of your network, you will find that having residual connections like those in ResNet architectures will be helpful\n",
    "* Optimizer: Instead of plain SGD, you may want to add a learning rate schedule, add momentum, or use one of the other optimizers you have learned about like Adam. Check the `torch.optim` package for other optimizers\n",
    "* Data augmentation: You should use the `torchvision.transforms` module to try adding random resized crops and horizontal flips of the input data. Check `transforms.RandomResizedCrop` and `transforms.RandomHorizontalFlip` for this. Feel free to apply more [transforms](https://pytorch.org/docs/stable/torchvision/transforms.html) for data augmentation which can lead to better performance. \n",
    "* Epochs: Once you have found a generally good hyperparameter setting try training for more epochs\n",
    "* Loss function: You might want to add weighting to the `MultiLabelSoftMarginLoss` for classes that are less well represented or experiment with a different loss function\n",
    "\n",
    "\n",
    "\n",
    "#### Note\n",
    "We will soon be providing some initial expectations of mAP values as a function of epoch so you can get an early idea whether your implementation works without waiting a long time for training to converge.\n",
    "\n",
    "### What to submit \n",
    "Submit your best model to Kaggle and save all plots for the writeup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\touba\\Desktop\\CS444-MP\\assignment3_part1\\voc_dataloader.py:137: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(box_indices),\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std= [0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "            transforms.Resize(227),\n",
    "            transforms.CenterCrop(227),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "            transforms.Resize(227),\n",
    "            transforms.CenterCrop(227),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "ds_train = VocDataset('VOCdevkit_2007/VOC2007/','train',train_transform)\n",
    "ds_val = VocDataset('VOCdevkit_2007/VOC2007/','val',test_transform)\n",
    "ds_test = VocDataset('VOCdevkit_2007/VOC2007test/','test', test_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "test_frequency = 5\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=ds_train,\n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True,\n",
    "                                               num_workers=1)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=ds_val,\n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True,\n",
    "                                               num_workers=1)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=ds_test,\n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=False,\n",
    "                                               num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch number 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2, 512, 14, 14]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\touba\\Desktop\\CS444-MP\\assignment3_part1\\MP3_P1B_Develop_Classifier.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment3_part1/MP3_P1B_Develop_Classifier.ipynb#ch0000012?line=8'>9</a>\u001b[0m \u001b[39m# optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment3_part1/MP3_P1B_Develop_Classifier.ipynb#ch0000012?line=10'>11</a>\u001b[0m init_checkpoint(PATH)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment3_part1/MP3_P1B_Develop_Classifier.ipynb#ch0000012?line=11'>12</a>\u001b[0m classifier, train_losses, val_losses, train_mAPs, val_mAPs \u001b[39m=\u001b[39m train(PATH, classifier, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency)\n",
      "\u001b[1;32mc:\\Users\\touba\\Desktop\\CS444-MP\\assignment3_part1\\MP3_P1B_Develop_Classifier.ipynb Cell 8'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(path, classifier, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment3_part1/MP3_P1B_Develop_Classifier.ipynb#ch0000007?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(initial_epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, num_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment3_part1/MP3_P1B_Develop_Classifier.ipynb#ch0000007?line=16'>17</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStarting epoch number \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(epoch))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment3_part1/MP3_P1B_Develop_Classifier.ipynb#ch0000007?line=17'>18</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train_classifier(train_loader, classifier, criterion, optimizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment3_part1/MP3_P1B_Develop_Classifier.ipynb#ch0000007?line=18'>19</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment3_part1/MP3_P1B_Develop_Classifier.ipynb#ch0000007?line=19'>20</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss for Training on Epoch \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(epoch) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m is \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(train_loss))\n",
      "\u001b[1;32mc:\\Users\\touba\\Desktop\\CS444-MP\\assignment3_part1\\MP3_P1B_Develop_Classifier.ipynb Cell 4'\u001b[0m in \u001b[0;36mtrain_classifier\u001b[1;34m(train_loader, classifier, criterion, optimizer)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment3_part1/MP3_P1B_Develop_Classifier.ipynb#ch0000003?line=7'>8</a>\u001b[0m logits \u001b[39m=\u001b[39m classifier(images)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment3_part1/MP3_P1B_Develop_Classifier.ipynb#ch0000003?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(logits, labels)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment3_part1/MP3_P1B_Develop_Classifier.ipynb#ch0000003?line=9'>10</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment3_part1/MP3_P1B_Develop_Classifier.ipynb#ch0000003?line=10'>11</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment3_part1/MP3_P1B_Develop_Classifier.ipynb#ch0000003?line=11'>12</a>\u001b[0m losses\u001b[39m.\u001b[39mappend(loss)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cs444\\lib\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cs444\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2, 512, 14, 14]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# TODO: Run your own classifier here\n",
    "PATH = 'convnet-bn.pt'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "classifier = ConvNet([2, 2, 3, 3, 3], bn=True, skip_connect=True, num_classes=21).to(device)\n",
    "\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "optimizer = torch.optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\n",
    "\n",
    "init_checkpoint(PATH)\n",
    "classifier, train_losses, val_losses, train_mAPs, val_mAPs = train(PATH, classifier, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(PATH)\n",
    "train_losses = checkpoint['train_losses']\n",
    "train_mAPs = checkpoint['train_mAPs']\n",
    "val_losses = checkpoint['val_losses']\n",
    "val_mAPs = checkpoint['val_mAPs']\n",
    "len(train_losses), len(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, test_frequency, num_epochs)\n",
    "plot_mAP(train_mAPs, val_mAPs, test_frequency, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP_test, test_loss, test_aps = test_classifier(test_loader, classifier, criterion)\n",
    "print(mAP_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), './voc_my_best_classifier.pth')\n",
    "output_submission_csv('my_solution.csv', test_aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
