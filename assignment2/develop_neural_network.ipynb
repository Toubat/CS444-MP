{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a Neural Network\n",
    "\n",
    "This notebook contains useful information and testing code to help you develop a neural network by implementing the forward pass and backpropagation algorithm in the `models/neural_net.py` file. \n",
    "\n",
    "You will implement your network in the class `NeuralNetwork` inside the file `models/neural_net.py` to represent instances of the network. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from models.neural_net import NeuralNetwork\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)  # set default size of plots\n",
    "\n",
    "# For auto-reloading external modules\n",
    "# See http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\"Returns relative error\"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below initializes a toy dataset and corresponding model which will allow you to check your forward and backward pass by using a numeric gradient check. Note that we set a random seed for repeatable experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model(num_layers):\n",
    "    \"\"\"Initializes a toy model\"\"\"\n",
    "    np.random.seed(0)\n",
    "    hidden_sizes = [hidden_size] * (num_layers - 1)\n",
    "    return NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers)\n",
    "\n",
    "def init_toy_data():\n",
    "    \"\"\"Initializes a toy dataset\"\"\"\n",
    "    np.random.seed(0)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.random.randint(num_classes, size=num_inputs)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement forward and backward pass\n",
    "\n",
    "The first thing you will do is implement the forward pass of your neural network. The forward pass should be implemented in the `forward` function. You can use helper functions like `linear`, `relu`, and `softmax` to help organize your code.\n",
    "\n",
    "Next, you will implement the backward pass using the backpropagation algorithm. Backpropagation will compute the gradient of the loss with respect to the model parameters `W1`, `b1`, ... etc. Use a softmax fuction with cross entropy loss for loss calcuation. Fill in the code blocks in `NeuralNetwork.backward`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient  check\n",
    "\n",
    "If you have implemented your forward pass through the network correctly, you can use the following cell to debug your backward pass with a numeric gradient check. If your backward pass has been implemented correctly, the max relative error between your analytic solution and the numeric solution should be around 1e-7 or less for all parameters.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "1. If you change the regularization coefficient to 0 and it works, but it doesn't work when your regularization coefficient is > 0, then there is a bug in your regularization code. Make sure both the loss and gradient calculations account for regularization correctly.\n",
    "2. If your bias vectors look good but your weight matrices don't, there is either a bug in your regularization loss or in your weight gradient calculation.\n",
    "3. If all of your parameters are incorrect, then there may be a bug in your gradient of the cross-entropy loss. See https://deepnotes.io/softmax-crossentropy.\n",
    "4. If you see numeric issues like underflow or division by zero, you may need to subtract the maximum element in your softmax. See https://deepnotes.io/softmax-crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from utils.gradient_check import eval_numerical_gradient\n",
    "\n",
    "X, y = init_toy_data()\n",
    "regularization = 0.05\n",
    "\n",
    "def f(W):\n",
    "    net.forward(X)\n",
    "    return net.backward(y, regularization)\n",
    "\n",
    "for num in [2, 3]:\n",
    "    net = init_toy_model(num)\n",
    "    net.forward(X)\n",
    "    net.backward(y, regularization)\n",
    "    gradients = deepcopy(net.gradients)\n",
    "\n",
    "    for param_name in net.params:\n",
    "        param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "        print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, gradients[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "To train the network we will use stochastic gradient descent (SGD), similar to the SVM and Softmax classifiers you trained. This should be similar to the training procedure you used for the SVM and Softmax classifiers.\n",
    "\n",
    "Once you have implemented the `update` method, run the code below to train a two-layer network on toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 1\n",
    "learning_rate = 1e-1\n",
    "learning_rate_decay = 0.95\n",
    "regularization = 5e-6\n",
    "\n",
    "# Initialize a new neural network model\n",
    "net = init_toy_model(2)\n",
    "\n",
    "# Variables to store performance for each epoch\n",
    "train_loss = np.zeros(epochs)\n",
    "train_accuracy = np.zeros(epochs)\n",
    "\n",
    "# For each epoch...\n",
    "for epoch in range(epochs):        \n",
    "    # Run the forward pass of the model to get a prediction and compute the accuracy\n",
    "    scores = net.forward(X)\n",
    "    pred = np.argmax(scores, axis=1)\n",
    "    train_accuracy[epoch] += (pred == y).sum()\n",
    "    \n",
    "    # Run the backward pass of the model to compute the loss, and update the weights\n",
    "    train_loss[epoch] += net.backward(y, regularization)\n",
    "    net.update(learning_rate)\n",
    "\n",
    "train_accuracy /= num_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented things correctly, you should rapidly see the loss decrease to 0 and the accuracy increase to 100%. Your final loss may not be exactly 0 due to regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function and train accuracy\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_loss)\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_accuracy)\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
