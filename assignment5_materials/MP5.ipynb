{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\touba\\anaconda3\\envs\\cs444\\lib\\site-packages (0.23.1)\n",
      "Requirement already satisfied: pyvirtualdisplay in c:\\users\\touba\\anaconda3\\envs\\cs444\\lib\\site-packages (3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in c:\\users\\touba\\anaconda3\\envs\\cs444\\lib\\site-packages (from gym) (4.11.3)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\touba\\anaconda3\\envs\\cs444\\lib\\site-packages (from gym) (1.21.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\touba\\anaconda3\\envs\\cs444\\lib\\site-packages (from gym) (2.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\touba\\anaconda3\\envs\\cs444\\lib\\site-packages (from gym) (0.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\touba\\anaconda3\\envs\\cs444\\lib\\site-packages (from importlib-metadata>=4.10.0->gym) (3.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] \n",
    "!pip3 install scikit-image \n",
    "!pip3 install pip install autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m atari_py.import_roms ./Roms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN, DQN_LSTM\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape # (210, 160, 3)\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\touba\\Anaconda3\\envs\\cs444\\lib\\site-packages\\skimage\\util\\dtype.py:226: DeprecationWarning: Converting `np.inexact` or `np.floating` to a dtype is deprecated. The current result is `float64` which is not strictly correct.\n",
      "  dtypeobj_out = np.dtype(dtype)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f942f99340>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display state\n",
    "plt.imshow(get_frame(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = False # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        \n",
    "        state = next_state\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = np.clip(reward, -1, 1) \n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n",
    "    \n",
    "    a = agent.memory.sample_mini_batch(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "sample = torch.randn(5, 4)\n",
    "\n",
    "print(sample.shape)\n",
    "print(np.stack(sample[:, 0], axis=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a DQN LSTM Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a DQN agent that uses LSTM rather than past frames as history. We augment the experience replay to contain previous few (state, action, reward, next states) tuples rather than just one (state, action, reward, next states) tuple. This previous few states will enable the LSTM encode the history. Training loop remains nearly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import LSTM_Agent\n",
    "double_dqn = False # set to True if using double DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 3.0   memory length: 247   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 3.0\n",
      "episode: 1   score: 7.0   memory length: 547   epsilon: 1.0    steps: 300    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2   score: 1.0   memory length: 717   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 3.6666666666666665\n",
      "episode: 3   score: 0.0   memory length: 841   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 2.75\n",
      "episode: 4   score: 0.0   memory length: 965   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\touba\\Desktop\\CS444-MP\\assignment5_materials\\memory.py:58: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sample = np.array(sample)\n",
      "c:\\Users\\touba\\Desktop\\CS444-MP\\assignment5_materials\\agent.py:151: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mini_batch = np.array(mini_batch).transpose() # (4, batch_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5   score: 2.0   memory length: 1183   epsilon: 0.9996356800000079    steps: 218    lr: 0.0001     evaluation reward: 2.1666666666666665\n",
      "episode: 6   score: 2.0   memory length: 1402   epsilon: 0.9992020600000173    steps: 219    lr: 0.0001     evaluation reward: 2.142857142857143\n",
      "episode: 7   score: 0.0   memory length: 1526   epsilon: 0.9989565400000227    steps: 124    lr: 0.0001     evaluation reward: 1.875\n",
      "episode: 8   score: 4.0   memory length: 1824   epsilon: 0.9983665000000355    steps: 298    lr: 0.0001     evaluation reward: 2.111111111111111\n",
      "episode: 9   score: 1.0   memory length: 1995   epsilon: 0.9980279200000428    steps: 171    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 10   score: 3.0   memory length: 2241   epsilon: 0.9975408400000534    steps: 246    lr: 0.0001     evaluation reward: 2.090909090909091\n",
      "episode: 11   score: 1.0   memory length: 2411   epsilon: 0.9972042400000607    steps: 170    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 12   score: 2.0   memory length: 2630   epsilon: 0.9967706200000701    steps: 219    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 13   score: 2.0   memory length: 2829   epsilon: 0.9963766000000787    steps: 199    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 14   score: 2.0   memory length: 3028   epsilon: 0.9959825800000872    steps: 199    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 15   score: 2.0   memory length: 3247   epsilon: 0.9955489600000966    steps: 219    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 16   score: 1.0   memory length: 3420   epsilon: 0.9952064200001041    steps: 173    lr: 0.0001     evaluation reward: 1.9411764705882353\n",
      "episode: 17   score: 2.0   memory length: 3640   epsilon: 0.9947708200001135    steps: 220    lr: 0.0001     evaluation reward: 1.9444444444444444\n",
      "episode: 18   score: 2.0   memory length: 3860   epsilon: 0.994335220000123    steps: 220    lr: 0.0001     evaluation reward: 1.9473684210526316\n",
      "episode: 19   score: 1.0   memory length: 4011   epsilon: 0.9940362400001295    steps: 151    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 20   score: 2.0   memory length: 4209   epsilon: 0.993644200000138    steps: 198    lr: 0.0001     evaluation reward: 1.9047619047619047\n",
      "episode: 21   score: 0.0   memory length: 4333   epsilon: 0.9933986800001433    steps: 124    lr: 0.0001     evaluation reward: 1.8181818181818181\n",
      "episode: 22   score: 3.0   memory length: 4601   epsilon: 0.9928680400001548    steps: 268    lr: 0.0001     evaluation reward: 1.8695652173913044\n",
      "episode: 23   score: 1.0   memory length: 4752   epsilon: 0.9925690600001613    steps: 151    lr: 0.0001     evaluation reward: 1.8333333333333333\n",
      "episode: 24   score: 3.0   memory length: 5000   epsilon: 0.992078020000172    steps: 248    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 25   score: 2.0   memory length: 5199   epsilon: 0.9916840000001805    steps: 199    lr: 0.0001     evaluation reward: 1.8846153846153846\n",
      "episode: 26   score: 1.0   memory length: 5351   epsilon: 0.9913830400001871    steps: 152    lr: 0.0001     evaluation reward: 1.8518518518518519\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\touba\\Desktop\\CS444-MP\\assignment5_materials\\MP5.ipynb Cell 23'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/MP5.ipynb#ch0000021?line=48'>49</a>\u001b[0m \u001b[39m# Start training after random sample generation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/MP5.ipynb#ch0000021?line=49'>50</a>\u001b[0m \u001b[39mif\u001b[39;00m(frame \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/MP5.ipynb#ch0000021?line=50'>51</a>\u001b[0m     agent\u001b[39m.\u001b[39;49mtrain_policy_net(frame)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/MP5.ipynb#ch0000021?line=51'>52</a>\u001b[0m     \u001b[39m# Update the target network only for Double DQN only\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/MP5.ipynb#ch0000021?line=52'>53</a>\u001b[0m     \u001b[39mif\u001b[39;00m double_dqn \u001b[39mand\u001b[39;00m (frame \u001b[39m%\u001b[39m update_target_network_frequency)\u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\touba\\Desktop\\CS444-MP\\assignment5_materials\\agent.py:161\u001b[0m, in \u001b[0;36mLSTM_Agent.train_policy_net\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/agent.py?line=158'>159</a>\u001b[0m rewards \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(mini_batch[\u001b[39m2\u001b[39m])\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/agent.py?line=159'>160</a>\u001b[0m rewards \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(rewards)\u001b[39m.\u001b[39mcuda() \u001b[39m# (batch_size)\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/agent.py?line=160'>161</a>\u001b[0m next_states \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mfloat32(history[:, \u001b[39m1\u001b[39;49m:, :, :]) \u001b[39m/\u001b[39m \u001b[39m255.\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/agent.py?line=161'>162</a>\u001b[0m next_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(next_states)\u001b[39m.\u001b[39mcuda() \u001b[39m# (batch_size, 20, 84, 84)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/agent.py?line=163'>164</a>\u001b[0m \u001b[39m# checks if the game is over\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = LSTM_Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "\n",
    "HISTORY_SIZE = 1\n",
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "    hidden = None\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            temp = np.float32(history[:1, :, :]) / 255.\n",
    "            action, hidden = agent.get_action(temp, hidden)\n",
    "        \n",
    "        state = next_state\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        \n",
    "        history[1, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = np.clip(reward, -1, 1) \n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= 1000):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        \n",
    "        score += reward\n",
    "        history[:1, :, :] = history[1:, :, :]\n",
    "        \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn_lstm.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(32, 4)\n",
    "a.max(1)[0].shape\n",
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4', render_mode=\"human\")\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state, HISTORY_SIZE + 1)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    # env.render()\n",
    "    show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    print(info)\n",
    "    terminal_state = check_live(life, info['lives'])\n",
    "        \n",
    "    life = info['lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "# display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([\n",
    "  [1, 2, 3],\n",
    "  [4, 5, 6],\n",
    "  [7, 8, 9]\n",
    "])\n",
    "\n",
    "idx = np.array([0, 1, 2])\n",
    "# I want to take first element of first row, second element of first row, and third element of first row\n",
    "np.take_along_axis(a, idx[:, None], axis=1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 20])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
