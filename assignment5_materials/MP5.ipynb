{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] \n",
    "!pip3 install scikit-image \n",
    "!pip3 install pip install autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m atari_py.import_roms ./Roms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN, DQN_LSTM\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape # (210, 160, 3)\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\touba\\Anaconda3\\envs\\cs444\\lib\\site-packages\\skimage\\util\\dtype.py:226: DeprecationWarning: Converting `np.inexact` or `np.floating` to a dtype is deprecated. The current result is `float64` which is not strictly correct.\n",
      "  dtypeobj_out = np.dtype(dtype)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16407a9c220>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display state\n",
    "plt.imshow(get_frame(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = False # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        \n",
    "        state = next_state\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = np.clip(reward, -1, 1) \n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n",
    "    \n",
    "    a = agent.memory.sample_mini_batch(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "sample = torch.randn(5, 4)\n",
    "\n",
    "print(sample.shape)\n",
    "print(np.stack(sample[:, 0], axis=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a DQN LSTM Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a DQN agent that uses LSTM rather than past frames as history. We augment the experience replay to contain previous few (state, action, reward, next states) tuples rather than just one (state, action, reward, next states) tuple. This previous few states will enable the LSTM encode the history. Training loop remains nearly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\touba\\Desktop\\CS444-MP\\assignment5_materials\\MP5.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/MP5.ipynb#ch0000018?line=29'>30</a>\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/MP5.ipynb#ch0000018?line=30'>31</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/MP5.ipynb#ch0000018?line=31'>32</a>\u001b[0m     action, hidden \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mget_action(np\u001b[39m.\u001b[39;49mfloat32(history[:\u001b[39m1\u001b[39;49m, :, :]) \u001b[39m/\u001b[39;49m \u001b[39m255.\u001b[39;49m, hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/MP5.ipynb#ch0000018?line=32'>33</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/MP5.ipynb#ch0000018?line=33'>34</a>\u001b[0m next_state, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\touba\\Desktop\\CS444-MP\\assignment5_materials\\agent.py:115\u001b[0m, in \u001b[0;36mLSTM_Agent.get_action\u001b[1;34m(self, state, hidden)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/agent.py?line=111'>112</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_action\u001b[39m(\u001b[39mself\u001b[39m, state, hidden\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/agent.py?line=112'>113</a>\u001b[0m     \u001b[39m### CODE ###\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/agent.py?line=113'>114</a>\u001b[0m     \u001b[39m# Nearly same as that for Agent\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/agent.py?line=114'>115</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m a, hidden\n",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "from agent import LSTM_Agent\n",
    "agent = LSTM_Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "\n",
    "HISTORY_SIZE = 1\n",
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "    hidden = None\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action, hidden = agent.get_action(np.float32(history[:1, :, :]) / 255., hidden)\n",
    "        state = next_state\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[1, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = np.clip(reward, -1, 1) \n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:1, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn_lstm.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordEpisodeStatistics as Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\touba\\Desktop\\CS444-MP\\assignment5_materials\\MP5.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/MP5.ipynb#ch0000023?line=0'>1</a>\u001b[0m display \u001b[39m=\u001b[39m Display(visible\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, size\u001b[39m=\u001b[39;49m(\u001b[39m300\u001b[39;49m, \u001b[39m200\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/MP5.ipynb#ch0000023?line=1'>2</a>\u001b[0m display\u001b[39m.\u001b[39mstart()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/MP5.ipynb#ch0000023?line=3'>4</a>\u001b[0m \u001b[39m# Load agent\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/touba/Desktop/CS444-MP/assignment5_materials/MP5.ipynb#ch0000023?line=4'>5</a>\u001b[0m \u001b[39m# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cs444\\lib\\site-packages\\pyvirtualdisplay\\display.py:54\u001b[0m, in \u001b[0;36mDisplay.__init__\u001b[1;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, retries, extra_args, manage_global_env, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/display.py?line=50'>51</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcls\u001b[39m:\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/display.py?line=51'>52</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39munknown backend: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend)\n\u001b[1;32m---> <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/display.py?line=53'>54</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obj \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/display.py?line=54'>55</a>\u001b[0m     size\u001b[39m=\u001b[39msize,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/display.py?line=55'>56</a>\u001b[0m     color_depth\u001b[39m=\u001b[39mcolor_depth,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/display.py?line=56'>57</a>\u001b[0m     bgcolor\u001b[39m=\u001b[39mbgcolor,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/display.py?line=57'>58</a>\u001b[0m     retries\u001b[39m=\u001b[39mretries,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/display.py?line=58'>59</a>\u001b[0m     use_xauth\u001b[39m=\u001b[39muse_xauth,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/display.py?line=59'>60</a>\u001b[0m     \u001b[39m# check_startup=check_startup,\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/display.py?line=60'>61</a>\u001b[0m     extra_args\u001b[39m=\u001b[39mextra_args,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/display.py?line=61'>62</a>\u001b[0m     manage_global_env\u001b[39m=\u001b[39mmanage_global_env,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/display.py?line=62'>63</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/display.py?line=63'>64</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cs444\\lib\\site-packages\\pyvirtualdisplay\\xvfb.py:44\u001b[0m, in \u001b[0;36mXvfbDisplay.__init__\u001b[1;34m(self, size, color_depth, bgcolor, use_xauth, fbdir, dpi, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/xvfb.py?line=40'>41</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fbdir \u001b[39m=\u001b[39m fbdir\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/xvfb.py?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dpi \u001b[39m=\u001b[39m dpi\n\u001b[1;32m---> <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/xvfb.py?line=43'>44</a>\u001b[0m AbstractDisplay\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/xvfb.py?line=44'>45</a>\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/xvfb.py?line=45'>46</a>\u001b[0m     PROGRAM,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/xvfb.py?line=46'>47</a>\u001b[0m     use_xauth\u001b[39m=\u001b[39;49muse_xauth,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/xvfb.py?line=47'>48</a>\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/xvfb.py?line=48'>49</a>\u001b[0m     extra_args\u001b[39m=\u001b[39;49mextra_args,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/xvfb.py?line=49'>50</a>\u001b[0m     manage_global_env\u001b[39m=\u001b[39;49mmanage_global_env,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/xvfb.py?line=50'>51</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cs444\\lib\\site-packages\\pyvirtualdisplay\\abstractdisplay.py:85\u001b[0m, in \u001b[0;36mAbstractDisplay.__init__\u001b[1;34m(self, program, use_xauth, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/abstractdisplay.py?line=81'>82</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipe_wfd \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/abstractdisplay.py?line=82'>83</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retries_current \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/abstractdisplay.py?line=84'>85</a>\u001b[0m helptext \u001b[39m=\u001b[39m get_helptext(program)\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/abstractdisplay.py?line=85'>86</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_displayfd \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-displayfd\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m helptext\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/abstractdisplay.py?line=86'>87</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_displayfd:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cs444\\lib\\site-packages\\pyvirtualdisplay\\util.py:13\u001b[0m, in \u001b[0;36mget_helptext\u001b[1;34m(program)\u001b[0m\n\u001b[0;32m      <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/util.py?line=5'>6</a>\u001b[0m cmd \u001b[39m=\u001b[39m [program, \u001b[39m\"\u001b[39m\u001b[39m-help\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/util.py?line=7'>8</a>\u001b[0m \u001b[39m# py3.7+\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/util.py?line=8'>9</a>\u001b[0m \u001b[39m# p = subprocess.run(cmd, capture_output=True)\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/util.py?line=9'>10</a>\u001b[0m \u001b[39m# stderr = p.stderr\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/util.py?line=10'>11</a>\u001b[0m \n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/util.py?line=11'>12</a>\u001b[0m \u001b[39m# py3.6 also\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/util.py?line=12'>13</a>\u001b[0m p \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mPopen(\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/util.py?line=13'>14</a>\u001b[0m     cmd,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/util.py?line=14'>15</a>\u001b[0m     stdout\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mPIPE,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/util.py?line=15'>16</a>\u001b[0m     stderr\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mPIPE,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/util.py?line=16'>17</a>\u001b[0m     shell\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/util.py?line=17'>18</a>\u001b[0m )\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/util.py?line=18'>19</a>\u001b[0m _, stderr \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mcommunicate()\n\u001b[0;32m     <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/site-packages/pyvirtualdisplay/util.py?line=20'>21</a>\u001b[0m helptext \u001b[39m=\u001b[39m stderr\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cs444\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=946'>947</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=947'>948</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=948'>949</a>\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m--> <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=950'>951</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=951'>952</a>\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=952'>953</a>\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=953'>954</a>\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=954'>955</a>\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=955'>956</a>\u001b[0m                         errread, errwrite,\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=956'>957</a>\u001b[0m                         restore_signals,\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=957'>958</a>\u001b[0m                         gid, gids, uid, umask,\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=958'>959</a>\u001b[0m                         start_new_session)\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=959'>960</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=960'>961</a>\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=961'>962</a>\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cs444\\lib\\subprocess.py:1420\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1417'>1418</a>\u001b[0m \u001b[39m# Start the process\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1418'>1419</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1419'>1420</a>\u001b[0m     hp, ht, pid, tid \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mCreateProcess(executable, args,\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1420'>1421</a>\u001b[0m                              \u001b[39m# no special security\u001b[39;49;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1421'>1422</a>\u001b[0m                              \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1422'>1423</a>\u001b[0m                              \u001b[39mint\u001b[39;49m(\u001b[39mnot\u001b[39;49;00m close_fds),\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1423'>1424</a>\u001b[0m                              creationflags,\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1424'>1425</a>\u001b[0m                              env,\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1425'>1426</a>\u001b[0m                              cwd,\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1426'>1427</a>\u001b[0m                              startupinfo)\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1427'>1428</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1428'>1429</a>\u001b[0m     \u001b[39m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1429'>1430</a>\u001b[0m     \u001b[39m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1432'>1433</a>\u001b[0m     \u001b[39m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1433'>1434</a>\u001b[0m     \u001b[39m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1434'>1435</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1435'>1436</a>\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   <a href='file:///c%3A/Users/touba/Anaconda3/envs/cs444/lib/subprocess.py?line=1436'>1437</a>\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
